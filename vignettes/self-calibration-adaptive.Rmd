---
title: "self-calibration-adaptive"
author: "Hyunji Moon, Shinyoung Kim"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE, warning=FALSE}
library(SBC)
library(cmdstanr)
library(parallel)
library(bayesplot)
library(posterior)
library(dplyr)
library(rstan)
library(future)
library(ggpubr)
library(rstanarm)
library(ggplot2)
library(mclust)
options(mc.cores = parallel::detectCores())
plan(multisession)
options(SBC.min_chunk_size = 5)
set.seed(1984)
```

We introduce a self-calibration algorithm. First, dap operator: $P \rightarrow P$ is $\hat{g}(f(\Phi X, U), \Phi X)$. Limiting to normal is inspected to verify its sublinearity and monotonicity. Then, based on this hyperparmeter self-calibration algorithm is suggested. Final improvement of calibration is compared with SBC rank plot.

```{R}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 80

# number of observations
nobs <- 10#2

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2
```

```{R}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 80

# number of observations
nobs <- 10#2

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2
```

# Inspecting DAP of binom-laplace 
```{R}
generator_binom <- function(lambda_mu, lambda_sigma, fixed_args){
  # fixed value across simulated datasets
  # experiment settings
  nobs <- fixed_args$nobs
  nsize <- fixed_args$nsize
  # modular settings
  link_type <- fixed_args$link_type
  
  # generate
  eta <- rnorm(1, mean = lambda_mu, sd=lambda_sigma)
  mu <- invtf_param_vec(eta, link_type = link_type)
  Y <- rbinom(nobs, size = nsize, prob = mu) 
  list(
    parameters = list(eta = eta),
    generated = list(nobs= nobs, nsize = nsize, link = link_type,
                     lambda_mu = lambda_mu, lambda_log_sigma = log(lambda_sigma), 
                     Y = Y)
  )
}

fixed_args_binom <- list(nobs = nobs, nsize = nsize, link_type = 1, nsims = nsims, ndraws = ndraws)

# step2: inferring posterior
rstan_binom_mod <- stan_model("models/binom-laplace.stan")
backend_binom_opt <- SBC_backend_rstan_optimizing(rstan_binom_mod, draws = ndraws)

calculate_dap <- function(mu, sigma, generator, backened, fixed_args){
  datasets <- generate_datasets(SBC_generator_function(generator, mu, sigma, fixed_args), n_datasets = fixed_args$nsims)
  sbc_result <- compute_results(datasets, backened, thin_ranks = 1)
  draws_eta <- c()
  for(fit in sbc_result$fits){
    samples <- SBC_fit_to_draws_matrix(fit)
    draws_eta <- c(draws_eta, posterior::extract_variable(samples, "eta"))
  }
  # assume normal for dap
  mu <- mean(draws_eta)
  sigma <- sd(draws_eta)
  return(list(mu=mu, sigma=sigma, draws_eta=draws_eta))
}

prior_dap_b <- list(mu = c(), sigma = c(), dap_mu = c(), dap_sigma = c(), mu_loss = c(), sigma_loss = c())
for (mu in seq(-5, 5, by = 1)){
  for(sigma in seq(1, 9, by = 1)){
    prior_dap_b$mu <- c(prior_dap_b$mu, mu)
    prior_dap_b$sigma <- c(prior_dap_b$sigma, sigma)
    dap <- calculate_dap(mu, sigma, generator_binom, backend_binom_opt, fixed_args_binom)
    prior_dap_b$dap_mu <- c(prior_dap_b$dap_mu, dap$mu)
    prior_dap_b$dap_sigma <- c(prior_dap_b$dap_sigma, dap$sigma)
    prior_dap_b$mu_loss <- c(prior_dap_b$mu_loss, mu - dap$mu)
    prior_dap_b$sigma_loss <- c(prior_dap_b$sigma_loss, sigma - dap$sigma)
  }
}
prior_dap_b <- data.frame(prior_dap_b)
ggplot(prior_dap_b)+  geom_point(aes(x=mu, y=sigma), color = "red") + geom_point(aes(x=dap_mu, y = dap_sigma), color = "blue")
```

# Inspecting DAP of gamma-regression-hmc
````{R}
generator_gr <- function(lambda_mu, lambda_sigma, fixed_args){
  # fixed value across simulated datasets
  ## meta
  nobs <- fixed_args$nobs
  K <- fixed_args$K
  shape <- fixed_args$shape
  # predictor
  X <- array(rnorm(nobs * K, mean = 1, sd = 1), dim = c(nobs, K))
  b <- rnorm(K, mean = 0, sd = 1)
  # generate
  eta <- rnorm(1, mean = lambda_mu, sd=lambda_sigma)
  logmu <- as.numeric(eta+ X %*% b)
  mu <- exp(logmu)
  Y <- rgamma(nobs, shape = shape, scale = mu/shape) 
  list(
    parameters = list(eta = eta),
    generated = list(nobs= nobs, K = K, X = X, shape = shape,
                          lambda_mu = lambda_mu, lambda_log_sigma = log(lambda_sigma), 
                          Y = Y)
  )
}

fixed_args_gr <- list(nobs = nobs, K = 15, shape = 1, nsims = nsims)
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = 4, iter_sampling = ndraws / 4) # thin = 10
prior_dap_gr <- list(mu = c(), sigma = c(), dap_mu = c(), dap_sigma = c(), mu_loss = c(), sigma_loss = c())
for (mu in seq(-3, 3, by = 1)){
  for(sigma in seq(1, 51, by = 10)){
    prior_dap_gr$mu <- c(prior_dap_gr$mu, mu)
    prior_dap_gr$sigma <- c(prior_dap_gr$sigma, sigma)
    dap <- calculate_dap(mu, sigma, generator_gr, backend_gr_hmc, fixed_args_gr)
    prior_dap_gr$dap_mu <- c(prior_dap_gr$dap_mu, dap$mu)
    prior_dap_gr$dap_sigma <- c(prior_dap_gr$dap_sigma, dap$sigma)
    prior_dap_gr$mu_loss <- c(prior_dap_gr$mu_loss, mu - dap$mu)
    prior_dap_gr$sigma_loss <- c(prior_dap_gr$sigma_loss, sigma - dap$sigma)
  }
}
prior_dap_gr <- data.frame(prior_dap_gr)
ggplot(prior_dap_gr)+  geom_point(aes(x=mu, y=sigma), color = "red") + geom_point(aes(x=dap_mu, y = dap_sigma), color = "blue")
```

# Self-calibration of binom-laplace
```{r, warning=FALSE, error=FALSE}
# initial prior hyperparameters
lambda_mu <- 100 #11000.7617#1
lambda_sigma <- 100 #1000 #exp(2.536367)#10

datasets_binom <- generate_datasets(SBC_generator_function(generator_binom, lambda_mu, lambda_sigma, fixed_args_binom), n_datasets = fixed_args_binom$nsims)

# hyperparameter update algorithm 
updator = "normal_str_update"

# maximal number of SBC iterations
niter <- 100

# tolerance
tol <- 0.1

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update

# step2: inferring posterior
rstan_binom_mod <- stan_model("models/binom-laplace.stan")
#cmdstan_binom_mod <- cmdstanr::cmdstan_model("models/binom-laplace.stan")

backend_binom_opt <- SBC_backend_rstan_optimizing(rstan_binom_mod, draws = ndraws)
#backend_binom_hmc <- SBC_backend_cmdstan_sample(cmdstan_binom_mod, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
result_binom_opt <- compute_results(datasets_binom, backend_binom_opt, thin_ranks = 1)
plot_rank_hist(result_binom_opt)
# result_binom_hmc <- compute_results(datasets_binom, backend_binom_hmc)

# step3: updating hyperparmeters
# wrapper function to follow `self_calib_adaptive` interface
calib_generator <- function(lambda_mu, lambda_sigma, fixed_args){
  generate_datasets(SBC_generator_function(generator_binom, lambda_mu, lambda_sigma, fixed_args), n_datasets = fixed_args$nsims)
}
sc_binom_opt <- self_calib_adaptive(calib_generator, backend_binom_opt, updator, "eta", lambda_mu, lambda_sigma, nsims, gamma, tol, fixed_args = fixed_args_binom)
datasets_binom_new <- generate_datasets(SBC_generator_function(generator_binom, sc_binom_opt$mu, sc_binom_opt$sigma, fixed_args_binom), n_datasets = fixed_args_binom$nsims)
result_binom_opt_new <- compute_results(datasets_binom_new, backend_binom_opt, thin_ranks = 1)
    
plot_rank_hist(result_binom_opt_new)
```

```{R}
# prior hyperparameters
lambda_mu <- 50
lambda_sigma <- 50
fixed_args_gamma <- list(nobs = nobs, K = 15, shape = 1, nsims = nsims)
datasets_gamma <- generate_datasets(SBC_generator_function(generator_gamma, lambda_mu, lambda_sigma, fixed_args_gamma), n_datasets = nsims)
calib_generator <- function(lambda_mu, lambda_sigma, fixed_args){
  generate_datasets(SBC_generator_function(generator_gamma, lambda_mu, lambda_sigma, fixed_args), n_datasets = fixed_args$nsims)
}
rstan_mod_gr <- stan_model("models/gamma-reg.stan")
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
# backend_gr_opt <- SBC_backend_rstan_optimizing(rstan_mod_gr, draws = ndraws)
# OPT DOES NOT WORK: result in Error in chol.default(-H) : the leading minor of order 13 is not positive definite

backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
result_gr_hmc <- compute_results(datasets_gamma, backend_gr_hmc)
plot_rank_hist(result_gr_hmc)

sc_opt_gr <- self_calib_adaptive(calib_generator, backend_gr_hmc, updator, "eta", lambda_mu, lambda_sigma, nsims, gamma, tol, fixed_args = fixed_args_gamma)

# after calibration
sc_opt_gr <- sc_opt
datasets_gr_new <- generate_datasets(SBC_generator_function(generator_gamma, sc_opt_gr$mu, sc_opt_gr$sigma, fixed_args_gamma), n_datasets = fixed_args_gamma$nsims)
result_gr_hmc_new <- compute_results(datasets_gr_new, backend_gr_hmc)
    
plot_rank_hist(result_gr_hmc_new)
```
