---
title: "self-calibration-adaptive"
author: "Hyunji Moon, Shinyoung Kim"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE, warning=FALSE}
library(SBC)
library(cmdstanr)
library(parallel)
library(bayesplot)
library(posterior)
library(dplyr)
library(rstan)
library(future)
library(ggpubr)
library(rstanarm)
library(ggplot2)
library(mclust)
options(mc.cores = parallel::detectCores())
plan(multisession)
options(SBC.min_chunk_size = 5)
set.seed(1984)
```

We introduce a self-calibration algorithm. First, dap operator: $P \rightarrow P$ is $\hat{g}(f(\Phi X, U), \Phi X)$. Limiting to normal is inspected to verify its sublinearity and monotonicity. Then, based on this hyperparmeter self-calibration algorithm is suggested. Final improvement of calibration is compared with SBC rank plot.

We finally apply the self-calibration algorithm to the centered eightschools model, starting from an unstable region and attempting to find a self-consistent region. We found that for models that impose constraints, reparameterizing the model to include an unconstrained parameter, run self calibration in the unconstrained space, and the applying constraints manually showed the best results.

```{R}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 80

# number of observations
nobs <- 10#2

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2
```

```{R}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 80

# number of observations
nobs <- 10#2

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2
```

# Inspecting DAP of binom-laplace 
```{R}
generator_binom <- function(lambda_mu, lambda_sigma, fixed_args){
  # fixed value across simulated datasets
  # experiment settings
  nobs <- fixed_args$nobs
  nsize <- fixed_args$nsize
  # modular settings
  link_type <- fixed_args$link_type
  
  # generate
  eta <- rnorm(1, mean = lambda_mu, sd=lambda_sigma)
  mu <- invtf_param_vec(eta, link_type = link_type)
  Y <- rbinom(nobs, size = nsize, prob = mu) 
  list(
    parameters = list(eta = eta),
    generated = list(nobs= nobs, nsize = nsize, link = link_type,
                     lambda_mu = lambda_mu, lambda_log_sigma = log(lambda_sigma), 
                     Y = Y)
  )
}

fixed_args_binom <- list(nobs = nobs, nsize = nsize, link_type = 1, nsims = nsims, ndraws = ndraws)

# step2: inferring posterior
rstan_binom_mod <- stan_model("models/binom-laplace.stan")
backend_binom_opt <- SBC_backend_rstan_optimizing(rstan_binom_mod, draws = ndraws)

calculate_dap <- function(mu, sigma, generator, backened, fixed_args){
  datasets <- generate_datasets(SBC_generator_function(generator, mu, sigma, fixed_args), n_datasets = fixed_args$nsims)
  sbc_result <- compute_results(datasets, backened, thin_ranks = 1)
  draws_eta <- c()
  for(fit in sbc_result$fits){
    samples <- SBC_fit_to_draws_matrix(fit)
    draws_eta <- c(draws_eta, posterior::extract_variable(samples, "eta"))
  }
  # assume normal for dap
  mu <- mean(draws_eta)
  sigma <- sd(draws_eta)
  return(list(mu=mu, sigma=sigma, draws_eta=draws_eta))
}

prior_dap_b <- list(mu = c(), sigma = c(), dap_mu = c(), dap_sigma = c(), mu_loss = c(), sigma_loss = c())
for (mu in seq(-5, 5, by = 1)){
  for(sigma in seq(1, 9, by = 1)){
    prior_dap_b$mu <- c(prior_dap_b$mu, mu)
    prior_dap_b$sigma <- c(prior_dap_b$sigma, sigma)
    dap <- calculate_dap(mu, sigma, generator_binom, backend_binom_opt, fixed_args_binom)
    prior_dap_b$dap_mu <- c(prior_dap_b$dap_mu, dap$mu)
    prior_dap_b$dap_sigma <- c(prior_dap_b$dap_sigma, dap$sigma)
    prior_dap_b$mu_loss <- c(prior_dap_b$mu_loss, mu - dap$mu)
    prior_dap_b$sigma_loss <- c(prior_dap_b$sigma_loss, sigma - dap$sigma)
  }
}
prior_dap_b <- data.frame(prior_dap_b)
ggplot(prior_dap_b)+  geom_point(aes(x=mu, y=sigma), color = "red") + geom_point(aes(x=dap_mu, y = dap_sigma), color = "blue")
```

# Inspecting DAP of gamma-regression-hmc
````{R}
generator_gr <- function(lambda_mu, lambda_sigma, fixed_args){
  # fixed value across simulated datasets
  ## meta
  nobs <- fixed_args$nobs
  K <- fixed_args$K
  shape <- fixed_args$shape
  # predictor
  X <- array(rnorm(nobs * K, mean = 1, sd = 1), dim = c(nobs, K))
  b <- rnorm(K, mean = 0, sd = 1)
  # generate
  eta <- rnorm(1, mean = lambda_mu, sd=lambda_sigma)
  logmu <- as.numeric(eta+ X %*% b)
  mu <- exp(logmu)
  Y <- rgamma(nobs, shape = shape, scale = mu/shape) 
  list(
    parameters = list(eta = eta),
    generated = list(nobs= nobs, K = K, X = X, shape = shape,
                          lambda_mu = lambda_mu, lambda_log_sigma = log(lambda_sigma), 
                          Y = Y)
  )
}

fixed_args_gr <- list(nobs = nobs, K = 15, shape = 1, nsims = nsims)
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = 4, iter_sampling = ndraws / 4) # thin = 10
prior_dap_gr <- list(mu = c(), sigma = c(), dap_mu = c(), dap_sigma = c(), mu_loss = c(), sigma_loss = c())
for (mu in seq(-3, 3, by = 1)){
  for(sigma in seq(1, 51, by = 10)){
    prior_dap_gr$mu <- c(prior_dap_gr$mu, mu)
    prior_dap_gr$sigma <- c(prior_dap_gr$sigma, sigma)
    dap <- calculate_dap(mu, sigma, generator_gr, backend_gr_hmc, fixed_args_gr)
    prior_dap_gr$dap_mu <- c(prior_dap_gr$dap_mu, dap$mu)
    prior_dap_gr$dap_sigma <- c(prior_dap_gr$dap_sigma, dap$sigma)
    prior_dap_gr$mu_loss <- c(prior_dap_gr$mu_loss, mu - dap$mu)
    prior_dap_gr$sigma_loss <- c(prior_dap_gr$sigma_loss, sigma - dap$sigma)
  }
}
prior_dap_gr <- data.frame(prior_dap_gr)
ggplot(prior_dap_gr)+  geom_point(aes(x=mu, y=sigma), color = "red") + geom_point(aes(x=dap_mu, y = dap_sigma), color = "blue")
```
# Inspecting DAP of eightschools
Since tau requires a positive constraint, we will be working with log(tau).
```{R}
generator_eightschools_cp <- function(lambda_mu, lambda_sigma, fixed_args){
  # fixed value across simulated datasets
  nsims <- fixed_args$nsims
  J <- fixed_args$J
  sigma <- fixed_args$sigma
  
  # Draw tau from the designated normal distribuiton
  a_bar <- -lambda_mu/lambda_sigma
  # tau <- NaN
  # while(TRUE){
  #   u <- runif(1)
  #   x_bar <- sqrt(a_bar^2 - 2 * log(1 - u))
  #   nu <- runif(1)
  #   tau <- lambda_sigma * x_bar + lambda_mu
  #   if(nu <= x_bar/a_bar) break
  # }
  log_tau <- rnorm(1, lambda_mu, lambda_sigma)
  tau <- exp(log_tau)
  
  
  
  # other parameters are drawn from the default prior
  mu = rnorm(1, 0, 5)
  theta <- rnorm(1, mu, tau)
  # draw y from simulated parameters
  y <- rnorm(J, mu, sigma)
  
  list(
    parameters = list(log_tau = log_tau), 
    generated = list(
      J = J,
      y = y,
      sigma = sigma,
      nsims = nsims,
      lambda_mu = lambda_mu,
      lambda_log_sigma = log(lambda_sigma)
    )
  )
}

cmdstan_mod_eightschools <- cmdstanr::cmdstan_model("models/eightschools_cp_posteriordb.stan")
backend_eightschools_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_eightschools, chains = 4, iter_sampling = ndraws / 4) # thin = 10
fixed_args_eightschools <- list(J = 8, nsims = nsims, sigma = c(15, 10, 16, 11, 9, 11, 10, 18), nsims=nsims)
datasets <- generate_datasets(SBC_generator_function(generator_eightschools_cp, 0, 1, fixed_args_eightschools), n_datasets = fixed_args_eightschools$nsims)
sbc_result <- compute_results(datasets, backend_eightschools_hmc, thin_ranks = 4)
plot_rank_hist(sbc_result)
```


# Self-calibration of binom-laplace
```{r, warning=FALSE, error=FALSE}
# initial prior hyperparameters
lambda_mu <- 100 #11000.7617#1
lambda_sigma <- 100 #1000 #exp(2.536367)#10

datasets_binom <- generate_datasets(SBC_generator_function(generator_binom, lambda_mu, lambda_sigma, fixed_args_binom), n_datasets = fixed_args_binom$nsims)

# hyperparameter update algorithm 
updator = "normal_str_update"

# maximal number of SBC iterations
niter <- 100

# tolerance
tol <- 0.1

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update

# step2: inferring posterior
rstan_binom_mod <- stan_model("models/binom-laplace.stan")
#cmdstan_binom_mod <- cmdstanr::cmdstan_model("models/binom-laplace.stan")

backend_binom_opt <- SBC_backend_rstan_optimizing(rstan_binom_mod, draws = ndraws)
#backend_binom_hmc <- SBC_backend_cmdstan_sample(cmdstan_binom_mod, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
result_binom_opt <- compute_results(datasets_binom, backend_binom_opt, thin_ranks = 1)
plot_rank_hist(result_binom_opt)
# result_binom_hmc <- compute_results(datasets_binom, backend_binom_hmc)

# step3: updating hyperparmeters
# wrapper function to follow `self_calib_adaptive` interface
calib_generator <- function(lambda_mu, lambda_sigma, fixed_args){
  generate_datasets(SBC_generator_function(generator_binom, lambda_mu, lambda_sigma, fixed_args), n_datasets = fixed_args$nsims)
}
sc_binom_opt <- self_calib_adaptive(calib_generator, backend_binom_opt, updator, "eta", lambda_mu, lambda_sigma, nsims, gamma, tol, fixed_args = fixed_args_binom)
datasets_binom_new <- generate_datasets(SBC_generator_function(generator_binom, sc_binom_opt$mu, sc_binom_opt$sigma, fixed_args_binom), n_datasets = fixed_args_binom$nsims)
result_binom_opt_new <- compute_results(datasets_binom_new, backend_binom_opt, thin_ranks = 1)
    
plot_rank_hist(result_binom_opt_new)
ggplot(sc_binom_opt$t_df) + geom_point(aes(x=iter, y=lambda_loss))
```

```{R}
# prior hyperparameters
lambda_mu <- 50
lambda_sigma <- 50
fixed_args_gamma <- list(nobs = nobs, K = 15, shape = 1, nsims = nsims)
datasets_gamma <- generate_datasets(SBC_generator_function(generator_gr, lambda_mu, lambda_sigma, fixed_args_gamma), n_datasets = nsims)
calib_generator <- function(lambda_mu, lambda_sigma, fixed_args){
  generate_datasets(SBC_generator_function(generator_gr, lambda_mu, lambda_sigma, fixed_args), n_datasets = fixed_args$nsims)
}
rstan_mod_gr <- stan_model("models/gamma-reg.stan")
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
# backend_gr_opt <- SBC_backend_rstan_optimizing(rstan_mod_gr, draws = ndraws)
# OPT DOES NOT WORK: result in Error in chol.default(-H) : the leading minor of order 13 is not positive definite

backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
result_gr_hmc <- compute_results(datasets_gamma, backend_gr_hmc)
plot_rank_hist(result_gr_hmc)

sc_opt_gr <- self_calib_adaptive(calib_generator, backend_gr_hmc, updator, "eta", lambda_mu, lambda_sigma, nsims, gamma, tol, fixed_args = fixed_args_gamma)

# after calibration
sc_opt_gr <- sc_opt
datasets_gr_new <- generate_datasets(SBC_generator_function(generator_Gr, sc_opt_gr$mu, sc_opt_gr$sigma, fixed_args_gamma), n_datasets = fixed_args_gamma$nsims)
result_gr_hmc_new <- compute_results(datasets_gr_new, backend_gr_hmc)
    
plot_rank_hist(result_gr_hmc_new)
ggplot(sc_binom_opt$t_df) + geom_point(aes(x=iter, y=lambda_loss))
```


We now repeat the above procedure with eight-school's log(tau). We start at N(10, 10), which upon inspection of the DAP distribution, results in a very narrow distribution near 0. Rank histogram plots also imply extreme underdispersion. But after 6 self-calibration iterations, we can observe the extreme rank plots being tamed, with the DAP distribution and the lambda hyperdistribution being similar.
```{R}
lambda_mu = 10
lambda_sigma = 10
cmdstan_mod_eightschools <- cmdstanr::cmdstan_model("models/eightschools_cp_posteriordb.stan")
backend_eightschools_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_eightschools, chains = 4, iter_sampling = ndraws / 4) # thin = 10
fixed_args_eightschools <- list(J = 8, nsims = nsims, sigma = c(15, 10, 16, 11, 9, 11, 10, 18), nsims=nsims)
datasets <- generate_datasets(SBC_generator_function(generator_eightschools_cp, 0, 1, fixed_args_eightschools), n_datasets = fixed_args_eightschools$nsims)
result_eightschools_hmc <- compute_results(datasets, backend_eightschools_hmc, thin_ranks = 4)
# initial badly calibrated
plot_rank_hist(result_eightschools_hmc)
calib_generator <- function(lambda_mu, lambda_sigma, fixed_args){
  generate_datasets(SBC_generator_function(generator_eightschools_cp, lambda_mu, lambda_sigma, fixed_args), n_datasets = fixed_args$nsims)
}

sc_hmc_eightschools <- self_calib_adaptive(calib_generator, backend_eightschools_hmc, updator, "log_tau", lambda_mu, lambda_sigma, nsims, gamma, tol, fixed_args = fixed_args_eightschools)

datasets_eightschools_new <- generate_datasets(SBC_generator_function(generator_eightschools_cp, sc_hmc_eightschools$mu, sc_hmc_eightschools$sigma, fixed_args_eightschools), n_datasets = fixed_args_eightschools$nsims)
result_eightschools_hmc_new <- compute_results(datasets_eightschools_new, backend_eightschools_hmc)
plot_rank_hist(result_eightschools_hmc_new)
# plot calibration result
```


