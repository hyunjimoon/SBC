---
title: "SBC for computational algorithm 1"
author: "Hyunji Moon, Martin ModrÃ¡k"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{SBC for computational algorithm 1}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---



# Summary 
Computational algorithms such as variational inference (VI) can fail due to the
inability of the approximation family to capture the true posterior, under/over
penalizing tendencies of convergence metric, and slow convergence of the
optimization process. We show two examples where VI is discouraged due to speed
(Zero-inflated Poisson) and accuracy (HMM) reasons. The trade-off is illustrated
by adjusting convergence-related parameters such as tolerance: longer run
achieves improved accuracy which sometimes becomes even slower than HMC. However,
when the fit between posterior and approximation family, convergence metric and
its process are checked so that efficiency is gained without sacrificing
accuracy too much, VI can be applied. On top of its role as "the test"
computational algorithms should pass, SBC provides informative inferential results
which directly affect workflow decisions.


# Introduction

HMC can be slow and depending on the joint posterior (as a combination of data,
prior, and likelihood) and the user's goal, deterministic approximation
algorithms can be an aid. To be specific, if the joint posterior is well-formed
enough for reliable approximation (symmetric for ADVI which has normal
approximation family) or the user only needs point estimate (i.e. specification
up to distribution-level is not needed) users can consider the deterministic
alternatives for their inference tool such as ADVI supported by Stan. Note that
Pathfinder (Zhang, 2021) which blends deterministic algorithm's efficiency and
stochastic algorithm's accuracy in a timely manner is under development. SBC
provides one standard to test whether ADVI works well for your model without
ever needing to run full HMC for your model. To show a few examples, let's start
by setting up our environment. For brevity, computational algorithm is shortened as computation.

```{r setup, message=FALSE,warning=FALSE, results="hide"}
library(SBC); 
library(ggplot2)
use_cmdstanr <- TRUE # Set to false to use rstan instead

if(use_cmdstanr) {
  library(cmdstanr)
} else {
  library(rstan)
}

options(mc.cores = parallel::detectCores())

# Parallel processing

library(future)
plan(multisession)

# The fits are very fast,
# so we force a minimum chunk size to reduce the overhead of
# paralellization and decrease computation time.
options(SBC.min_chunk_size = 5)


# Setup caching of results
cache_dir <- "./approximate_computation_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

```


# Example I - Poisson mixture

In this example, we have collected a set of counts of particles emitted by 
a specimen in a relatively large number of experimental runs. We however noticed 
that there is a suspiciously large number of low counts. Inspecting the 
equipment, it turns out that the experiment was not set up properly and
in some of the runs, our detector could only register background noise.
We however don't know which runs were erroneous. 

This naturally calls for a mixture model, where we assume that a proportion 
(`p_signal`) of experiments
contains both background noise and the signal of interest and the rest
contains just the background. For simplicity, we assume a Poisson distribution
for the counts. 
We also assume that in our hypothetical example, the expected background noise
is well understood, which lets us to put a narrow prior on the amount of 
particles we would expect to detect just from the background.

So here's our model:

```{r, comment = ""}
cat(readLines("stan/contaminated_poisson.stan"), sep = "\n")
```


And here's R code that generates data matching that model:

```{r}
generator_contamination <- function(N) {
  mu_background <- rlnorm(1, -2, 0.2)
  mu_signal <- rlnorm(1, 2, 1)

  p_signal <- rbeta(1, 2, 2)

  mu <- c(mu_background, mu_background + mu_signal)
  y <- rpois(N, ifelse(runif(N) < p_signal, mu_background + mu_signal, mu_background))
  
  list(
    parameters = list(
      mu_signal = mu_signal,
      mu_background = mu_background,
      p_signal = p_signal
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}
```

## Default ADVI (small problems)

We'll start with Stan's ADVI with all default parameters, i.e. a mean-field
variational approximation. 
We compile the model and create a variational SBC backend.

```{r}
model_contamination <- cmdstan_model("stan/contaminated_poisson.stan")
backend_contamination <- SBC_backend_cmdstan_variational(model_contamination, n_retries_init = 3)
```

Note that we allow the backend to retry initialization several times (`n_retries_init`),
as the ADVI implementation in Stan can sometimes fail to start properly on the first try.


Since we are feeling confident that our model is implemented correctly (and the model runs quickly), we start with 100 simulations and assume 100 observations for each.
If you are developing a new model, it might be useful to start with fewer simulations, as discussed in the 
[small model workflow vignette](https://hyunjimoon.github.io/SBC/articles/small_model_workflow.html).

Throughout the vignette, we'll also use caching for the results.

```{r}
set.seed(46522641)
ds_contamination <- generate_datasets(SBC_generator_function(generator_contamination, N = 100), n_datasets = 100)
res_contamination <- compute_results(ds_contamination, backend_contamination, cache_mode = "none", cache_location = file.path(cache_dir, "contamination"))
```
The ECDF diff plot and the ranks plot look quite good, although we can notice that the `mu_signal` parameter 
has a bit too many large ranks - the resolution we get from 100 simulations is
however too low to be sure if that's a problem or just a fluke.

```{r ecdf_rank_contamination}
plot_ecdf_diff(res_contamination)
plot_rank_hist(res_contamination)
```

We'll also look at how well are we able to estimate the various parameters by
plotting the estimated values + uncertainty intervals against the "true" simulated
values. We have a lot of data and both the `mu_signal` and `p_signal` are well
constrained by the data. Unsurprisingly we learn a bit less about `mu_background`,
because a) we put a strong prior on it and b) it requires a lot of observations 
to learn the precise rate of rare events as most observations will be just 0.


```{r contamination_sim_estimated}
plot_sim_estimated(res_contamination)
```

To understand how large problems could still be lurking in the model/approximation,
we can also show the observed coverage of central uncertainty intervals:

```{r }
plot_coverage(res_contamination)
```

This doesn't show a big problem, but we see that the remaining uncertainty is still 
quite large. 

```{r}
coverage_contamination <- empirical_coverage(res_contamination$stats, width = 0.9)[, c("parameter", "ci_low", "ci_high")]
coverage_contamination
```
For example for the 90% central, the 90% interval for `mu_signal` could actually
contain between `r scales::percent(coverage_contamination$ci_low[2])` and 
`scales::percent(coverage_contamination$ci_high[2])` of true values.

So we ran 900 more simulations and combine them with the previous results.


```{r}
set.seed(12365455)
ds_contamination_2 <- generate_datasets(SBC_generator_function(generator_contamination, N = 100), n_datasets = 900)
res_contamination_more <- compute_results(ds_contamination_2, backend_contamination, 
                  keep_fits = FALSE,
                  cache_mode = "results", 
                  cache_location = file.path(cache_dir, "contamination_2"))

res_contamination_2 <- bind_results(
  res_contamination,
  res_contamination_more
  )

  
```

It turns out there is a problem:

```{r contamination_2_ecdf_rank}
plot_ecdf_diff(res_contamination_2)
plot_rank_hist(res_contamination_2)
```

The increased resolution of a total of 1000 simulations shows that the variational
approximation to the posterior is under dispersed for `mu_signal` (the true parameter is too often
in the extreme tails of the posterior) and - to a lesser extent - also for `p_signal`,
where we see an overabundance of true values larger than all of the posterior samples,
but not too many low ranks.

Note that when the user is aiming for a point estimate of either mean or certain
quantile, a summary of VI posterior provides a good point estimate even when it
is far from the true posterior. VSBC, an extended diagnostic that concentrates
on bias in marginal quantity, is developed (Yao et. al., 2018). Other
diagnostic such as PSIS-based which is associated with specific data and test
quantity, is less flexible for target-testing.

To see how big of a problem this introduces, let's look at the coverage plot.
We see that for `mu_signal` the observed coverage is noticeably lower than
expected - for `p_signal` this seems to be much less of a problem.

```{r}
plot_coverage(res_contamination_2)
```

This is what we get when we focus on the 90% interval once again:

```{r}
coverage_contamination_2 <- empirical_coverage(res_contamination_2$stats, width = 0.9)[, c("parameter", "ci_low", "ci_high")]
coverage_contamination_2
```

The 90% central credible interval for `mu_background` likely
contains less than `r scales::percent(coverage_contamination$ci_high[2])` of true values.

So for a crude result, the default ADVI setup we just tested is not terrible: we don't
expect to see a strong bias and the model will be somewhat overconfident, but
not catastrophically so.

## Full-rank approximation (smaller problems)

We may also try the more flexible full-rank variational approximation.

So we next build a full-rank variational backend and compute SBC
for the same 1000 datasets we used earlier. This is primarily for brevity -
in actual practice, you would probably want to start with fewer datasets to
discover big problems quickly.

```{r}
res_contamination_fullrank <- compute_results(
  bind_datasets(ds_contamination, ds_contamination_2), 
  SBC_backend_cmdstan_variational(model_contamination, n_retries_init = 3, algorithm = "fullrank"),
  keep_fits = FALSE,
  cache_mode = "results",
  cache_location = file.path(cache_dir, "contamination_fullrank"))
```
The ECDF and rank plots actually look a bit better than with the mean-field approximation.
Interestingly, the rank plots show a bit of a "frowning" shape, meaning the
mean-field approximation is slightly underconfident for `p_signal` and a bit also
for `mu_background`. For `mu_signal`, the model is both slightly underconfident 
in the bulk of the posterior, but still has overly light tails and is overconfident
for the extreme quantiles.

```{r contamination_fullrank_ecdf_rank}
plot_ecdf_diff(res_contamination_fullrank)
plot_rank_hist(res_contamination_fullrank)

```

The coverage of the central intervals is still visibly imperfect, but
the imprecisions are smaller:

```{r contamination_fullrank_coverage}
plot_coverage(res_contamination_fullrank)
```

## Meanfield + lower tolerance (even better)

In some cases, it might also help to reduce the tolerance (`tol_rel_obj`) of the
algorithm. This is a restriction on evidence lower bound (ELBO) for tighter
optimization convergence. Here we'll use the default mean-field
algorithm, but decrease the `tol_rel_obj` (the default value is 0.01).

```{r}
res_contamination_lowtol <- compute_results(
  bind_datasets(ds_contamination, ds_contamination_2), 
  SBC_backend_cmdstan_variational(model_contamination, n_retries_init = 3, 
                                  tol_rel_obj = 0.001
                                  ),
  keep_fits = FALSE,
  cache_mode = "results",
  cache_location = file.path(cache_dir, "contamination_lowtol"))
```
Reducing tolerance leads to a small proportion of non-converging fits. In
theory, increasing `grad_samples` improve non-convergence but in our experience,
current ADVI (2021) convergence does not easily change with this adjustment.
Also, since the non-converged cases are relatively rare, we'll just remove the
non-converging fits from the SBC results (this is OK as long as we would discard
non-converging fits for real data, see the [rejection sampling
vignette](https://hyunjimoon.github.io/SBC/articles/rejection_sampling.html)).

```{r}
res_contamination_lowtol_conv <- res_contamination_lowtol[res_contamination_lowtol$backend_diagnostics$elbo_converged] 
```

Let's look at the ECDF and rank plots. We see some overabundance of very large ranks
for `mu_signal` and `p_signal` and somewhat underconfident bulk of `p_signal`,
but the problems are even smaller than before.

```{r contamination_lowtol_ecdf_rank}
plot_ecdf_diff(res_contamination_lowtol_conv)
plot_rank_hist(res_contamination_lowtol_conv)
```
The empirical coverage also tracks the expected coverage quite nicely across
the whole range of credible interval widths.

```{r contamination_lowtol_coverage}
plot_coverage(res_contamination_lowtol)
```


## Summary


Overall, the ADVI approximations for this model can be made to work well
and are not completely useless even on the default settings. 
As we'll see in the next example, this cannot be taken for granted.

However, have we actually gained speed by using an approximation?

To have a comparison, we'll run Stan's HMC as well, but just for the
first 50 datasets. 
We'll assume we have multiple cores, so the fact that Stan runs 4 chains doesn't
bother us - we'll just look at the time the longest chain took.

```{r}
res_contamination_sample <- compute_results(
  ds_contamination[1:50], 
  SBC_backend_cmdstan_sample(model_contamination),
  keep_fits = FALSE,
  cache_mode = "results",
  cache_location = file.path(cache_dir, "contamination_sample"))
```

We've also run the sampling for more datasets and promise the rank
and ECDF plots it produces are very nice :-)

For the machine we built the vignette on, here are the distributions
of times (for ADVI) and time of longest chain (for HMC):

```{r}
contamination_time <- 
  rbind(data.frame(alg = "Meanfield", time = res_contamination_2$backend_diagnostics$time),
        data.frame(alg = "Fullrank", time = res_contamination_fullrank$backend_diagnostics$time),
        data.frame(alg = "Meanfield + low tol.", time = res_contamination_lowtol$backend_diagnostics$time),
        data.frame(alg = "Sampling (longest chain)", time = res_contamination_sample$backend_diagnostics$max_chain_time))
 
ggplot(contamination_time, aes(x = time)) + geom_histogram(aes(y = ..density..)) + facet_wrap(~alg, ncol = 1) +
  scale_x_continuous("Time [seconds]")
```

Depressingly, while using lower tolerance let us get almost as good uncertainty
quantification as sampling, it also erased most of the performance advantage variational
inference had over sampling for this simple model.

# Example II - Hidden Markov Model

Continuing from our first example, let's say we realized that in fact
observing background only vs. signal in individual data points is not independent
and we want to model how the experimental setup switches between these two states over
time. We add additional structure to the model to account for this autocorrelation.

One possible choice for such structure is hidden Markov models (HMMs) where
we assume the probability of transitioning from one state to another is identical
across all time points. The [case study for HMMs](https://mc-stan.org/users/documentation/case-studies/hmm-example.html)
has a more thorough discussion and also shows how to code those in Stan.

Maybe the simplest way to describe the model is to show how we simulate the data:

```{r}
generator_HMM <- function(N) {
  
  # Rejection sampling for ordered mu with the correct priors
  repeat {
    mu <- c(rlnorm(1, 1, 1), rlnorm(1, 2, 1))
    if(mu[1] < mu[2]) {
      break;
    }
  }

  # Draw the transition probabilities
  t1 <- MCMCpack::rdirichlet(1, c(3, 3))
  t2 <- MCMCpack::rdirichlet(1, c(3, 3))

  states = rep(NA_integer_, N)
  # Draw from initial state distribution
  rho <- MCMCpack::rdirichlet(1, c(1, 10))

  states[1] = sample(1:2, size = 1, prob = rho)
  for(n in 2:length(states)) {
    if(states[n - 1] == 1)
      states[n] = sample(c(1, 2), size = 1, prob = t1)
    else if(states[n - 1] == 2)
      states[n] = sample(c(1, 2), size = 1, prob = t2)
  }  
  
  y <- rpois(N, mu[states])
  
  list(
    parameters = list(
      mu = mu,
      # rdirichlet returns matrices, convert to 1D vectors
      t1 = as.numeric(t1),
      t2 = as.numeric(t2),
      rho = as.numeric(rho)
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}
```

And here is the Stan code that models this process (it is based on the example
from the HMM case study but simplified and modified).

```{r, comment = ""}
cat(readLines("stan/hmm_contaminated_poisson.stan"), sep = "\n")
```

## Fitting ADVI

Once again, we build a default (meanfield) variational backend via Stan:

```{r}
model_HMM <- cmdstan_model("stan/hmm_contaminated_poisson.stan")
backend_HMM <- SBC_backend_cmdstan_variational(model_HMM, n_retries_init = 3)
```

And compute results - starting with just 20 datasets:

```{r}
set.seed(464855)
ds_hmm <- generate_datasets(SBC_generator_function(generator_HMM, N = 15), n_datasets = 200)
res_hmm <- compute_results(ds_hmm, backend_HMM,
                           cache_mode = "results", cache_location = file.path(cache_dir, "hmm"))
```
Already with 20 datasets, we see huge problems for most parameters:

```{r hmm_ecdf_ranks}
plot_ecdf_diff(res_hmm)
plot_rank_hist(res_hmm)
```

```{r}
plot_coverage(res_hmm)
```
```{r}
model_HMM2 <- cmdstan_model("stan/hmm_contaminated_poisson_2.stan")
backend_HMM2 <- SBC_backend_cmdstan_variational(model_HMM, n_retries_init = 3)
```

And compute results - starting with just 20 datasets:

```{r}
res_hmm2 <- compute_results(ds_hmm, backend_HMM2,
                           cache_mode = "results", cache_location = file.path(cache_dir, "hmm2"))
```

```{r}
plot_ecdf_diff(res_hmm2)
plot_rank_hist(res_hmm2)
```


We may also try full-rank - let's run it for the same 20 datasets.

```{r}
res_hmm_fullrank <- compute_results(
  ds_hmm, 
  SBC_backend_cmdstan_variational(model_HMM, algorithm = "fullrank", n_retries_init = 3),
  cache_mode = "results", cache_location = file.path(cache_dir, "hmm_fullrank"))
```

```{r}
plot_coverage(res_hmm_fullrank)

```


Once again, the calibration is very bad.

```{r hmm_fullrank_ecdf_ranks}
plot_ecdf_diff(res_hmm_fullrank)
plot_rank_hist(res_hmm_fullrank)
```

And we may also try mean-field but with lower tolerance

```{r}
res_hmm_lowtol <- compute_results(
  ds_hmm, 
  SBC_backend_cmdstan_variational(model_HMM, tol_rel_obj = 0.001, n_retries_init = 3),
  cache_mode = "results", cache_location = file.path(cache_dir, "hmm_lowtol"))
```

But the problems don't go away.

```{r hmm_lowtol_ecdf_ranks}
plot_ecdf_diff(res_hmm_lowtol)
plot_rank_hist(res_hmm_lowtol)
```

```{r}
plot_coverage(res_hmm_lowtol)
```
```{r}
empirical_coverage(res_hmm_lowtol$stats, width = c(0.8,0.9,0.95))
```


```{r}
res_hmm_sample <- compute_results(
  ds_hmm, 
  SBC_backend_cmdstan_sample(model_HMM),
  cache_mode = "results", cache_location = file.path(cache_dir, "hmm_sample"))
```

```{r hmm_sample_ecdf_ranks}
plot_ecdf_diff(res_hmm_sample)
plot_rank_hist(res_hmm_sample)
```

In comparison, HMC works well.

```{r}
plot_sim_estimated(res_hmm)
```


## Summary

To summarise, the HMM model turns out to pose big problems for all ADVI
variants we tested and inferences drawn from using ADVI on such a model
are likely useless.


# Next step: Evolving computation and diagnostic.
In computational_algorithm2, we will focus on hopeful aspects of approximate
computation. The adversarial relation between computation and diagnostic is
introduced based on which mutual evolvement happens. This can give insight to
computational algorithm designers aiming to pass SBC. For illustration, when and
how VI can be used is discussed which include customized SBC (e.g. VSBC) and first or
second-order correction.

# References
- Zhang et. al. (2021) Pathfinder: Parallel quasi-Newton variational inference https://arxiv.org/abs/2108.03782
- Turner and Sahani (2010) Two problems with variational expectation maximisation for
time-series models http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf
- Yao et. al. (2018) Yes, but Did It Work?: Evaluating Variational Inference http://proceedings.mlr.press/v80/yao18a/yao18a.pdf
