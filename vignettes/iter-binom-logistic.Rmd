---
title: "iter-binomial-laplace"
output: html_document
author: Hyunji Moon
---
```{r setup, include=FALSE}
library(SBC)
library(cmdstanr)
library(parallel)
library(bayesplot)
library(posterior)
library(dplyr)
library(rstan)
library(future)
library(ggpubr)
library(mclust)
library(rstanarm)
library(ggplot2)
options(mc.cores = parallel::detectCores())
plan(multisession)
options(SBC.min_chunk_size = 5)
set.seed(1984)
```

Proposition: SBC iteration would converge to the distribution that respects computation model.

This is due to the recurrence of well-calibrated regions which would be illustrated in experiment 1 which shows different priors converging to the same distribution. We view this as model bootstrap where likelihood and inference algorithm form a transition and automatically finds the best prior pair. Experiment 2 introduces a quantile-based hyperparameter gradient update which fastens this converge. The convergence is shown in 1-Wasserstein distance. Further attemps are applying this calibration boost algorithm to different likelihood and inference algorithm pairs which are known to have selective well-calibrated parameter region. Examples are as follows:

variance parameter starting from boundary-mode distribution 
- likelihood: hierarchical
- inference algorithm: HMC, variantional inference

coefficient parameter starting from fat-tailed distribution 
- likelihood: logit-link generalized linear model
- inference algorithm: HMC, variantional inference

# Experient 1.

Target parameter is logit-transformed probability, $a$. Binomial likelihood and laplace approximation inference algorithm on logit scale is used. Hyperparameters for laplace approximation are $\mu, \sigma$ which correspond to posterior distribution mode and second derivative at the mode. These hyperparameter values are set as the prior parameter for the iteration. Results show starting from $N(0, 1^2)$ distribution, initial non-normal distribution slowly transforms to normal form to adjust to the constraints imposed by the approximation of inference algorithm, in this case normal distribution.

```{r}
model = stan_model("./models/binom-laplace.stan")
SBC_iter <- 30
# prior hyperparameters
mu <- 0
sigma <- 1
mu_lst <- list()
sigma_lst <- list()
# the number of dataset
nsims <- 100
# outcome dimension for each dataset
nobs <- 2
# posterior samples for each dataset
ndraws <- 10
# number of binomial trials
nsize <- 2
for (j in 1:SBC_iter){
  post_draws_a <- c()
  a <- rnorm(nsims, mu, sigma)
  for (i in 1:nsims) {
  	p <- invlogit(a[i])
  	y <- rbinom(nobs, nsize, p)
  	dat <- list(Y=as.array(y), nsize=nsize, nobs=nobs, mu = mu, sigma = sigma)
  	fit <- optimizing(model, data = dat, hessian = TRUE)
  	
  	# approximate posterior mean via posterior mode
  	post_mean_a <- fit$par["a"]
  	
  	# approximate posterior sd via (sqrt) of the inverse negative Hessian
  	post_sd_a <- sqrt(solve(-fit$hessian))
  	post_draws_a <- c(post_draws_a, rnorm(ndraws, post_mean_a, post_sd_a))
  }
  if (j %% 5){
    hist(invlogit(post_draws_a), xlim = range(0,1))  
  }
  
  # update hyperparameters depending on inference algorithm
  mu_est <- mean(post_draws_a)
  mu <- mu_est
  sigma_est <- sd(post_draws_a)
  sigma <- sigma_est
  # compare with previous hyperparameters
  message("mu : ", round(mu, 2), " mu_est : ", round(mu_est, 2), " sigma : ", round(sigma, 2), " sigma_est : ", round(sigma_est, 2))
  mu_lst <- c(mu_lst, mu)
  sigma_lst <- c(sigma_lst, sigma)
}
plot(unlist(mu_lst))
plot(unlist(sigma_lst))
```

The second experiment with different prior, $N(5, 1^2)$ which is highly unsymmetrical unlike $N(0, 1^2)$, also converge to the same distribution. A likeable explanation is the recurrence of parameter values within well-calibrated region as opposed to that are not. For instance, if $logit(p)$ starts from 0.9, is likely to form a non-normal posterior whose mode is more likely to move away from 0.9. On the other hand, parameter values near .5 forms a symmetric and stable posterior which in most cases has its mode near .5. 

# Experiment 2.
```{r}
# change prior hyperparameters
mu <- 5
sigma <- 1
mu_lst <- list()
sigma_lst <- list()
for (j in 1:SBC_iter){
  post_draws_a <- c()
  a <- rnorm(nsims, mu, sigma)
  for (i in 1:nsims) {
  	p <- invlogit(a[i])
  	y <- rbinom(nobs, nsize, p)
  	dat <- list(Y=as.array(y), nsize=nsize, nobs=nobs, mu = mu, sigma = sigma)
  	fit <- optimizing(model, data = dat, hessian = TRUE)
  	
  	# approximate posterior mean via posterior mode
  	post_mean_a <- fit$par["a"]
  	
  	# approximate posterior sd via (sqrt) of the inverse negative Hessian
  	post_sd_a <- sqrt(solve(-fit$hessian))
  	post_draws_a <- c(post_draws_a, rnorm(ndraws, post_mean_a, post_sd_a))
  }
  if (j %% 5){
    hist(invlogit(post_draws_a), xlim = range(0,1))  
  }
  
  # update hyperparameters depending on inference algorithm
  mu_est <- mean(post_draws_a)
  mu <- mu_est
  sigma_est <- sd(post_draws_a)
  sigma <- sigma_est
  # compare with previous hyperparameters
  message("mu : ", round(mu, 2), " mu_est : ", round(mu_est, 2), " sigma : ", round(sigma, 2), " sigma_est : ", round(sigma_est, 2))

  mu_lst <- c(mu_lst, mu)
  sigma_lst <- c(sigma_lst, sigma)
}
plot(unlist(mu_lst))
plot(unlist(sigma_lst))
```
