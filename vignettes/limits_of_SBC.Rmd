---
title: "Limits of SBC"
output: html_notebook
---

```{r setup, include = FALSE}
library(SBC)
library(ggplot2)
use_cmdstanr <- TRUE # Set to false to use rstan instead

if(use_cmdstanr) {
  library(cmdstanr)
} else {
  library(rstan)
}

options(mc.cores = parallel::detectCores())

library(future)
plan(multisession)

# The fits are very fast,
# so we force a minimum chunk size to reduce overhead of
# paralellization and decrease computation time.
options(SBC.min_chunk_size = 5)


```


# SBC and minor changes to model

SBC requires _a lot_ of iterations to discover problems that are subtle.

To demonstrate this, we will fit a simple model with a normal likelihood, but use Student's t distribution with 5 degrees of freedom to generate the data.


To see the difference we'll show the two densities

```{r}
x <- seq(-5, 5, length.out = 100)
dens_data <- rbind(
  data.frame(x = x, density = dnorm(x, log = FALSE), 
             log_density = dnorm(x, log = TRUE), type = "normal()"),
  data.frame(x = x, density = dt(x, df = 5, log = FALSE), 
             log_density = dt(x, df = 5, log = TRUE), type = "t(5)")) 

ggplot(dens_data, aes(x = x, y = density, color = type)) +
  geom_line(size = 2)
```
As expected the t distribution has fatter tails, which is even better visible when looking at the logarithm of the density.

```{r}
ggplot(dens_data, aes(x = x, y = log_density, color = type)) +
  geom_line(size = 2)
```
Here is our Stan code for the simple normal model.

```{r}
model_code_minor <- "
data {
  int<lower=0> N;
  vector[N] y;
}

parameters {
  real mu;
  real<lower=0> sigma;
}

model {
  target += normal_lpdf(mu | 0, 1);
  target += normal_lpdf(sigma | 0, 1);
  target += normal_lpdf(y | mu, sigma);
}
"

iter_warmup <- 300
iter_sampling <- 1000

if(use_cmdstanr) {
  model_minor <- cmdstan_model(write_stan_file(model_code_minor))

  backend_minor <- SBC_backend_cmdstan_sample(
    model_minor, iter_warmup = iter_warmup, iter_sampling = iter_sampling, chains = 1)
} else {
  model_minor <- stan_model(model_code = model_code_minor)

  backend_minor <- SBC_backend_rstan_sample(
    model_minor, iter = iter_sampling + iter_warmup, warmup = iter_warmup, chains = 1)
}
```

And here we simulate from a student's t distribution. We scale the distribution so that the `sigma` parameter
is the standard deviation of the distribution.

```{r}
single_dataset_minor <- function(N) {
  mu <- rnorm(n = 1, mean = 0, sd = 1)
  sigma <- abs(rnorm(n = 1, mean = 0, sd = 1))
  nu <- 5
  student_scale <- sigma / sqrt(nu / (nu - 2))
  y <- mu + student_scale * rt(N, df = nu)
  
  list(
    parameters = list(mu = mu, sigma = sigma),
    generated = list(N = N, y = y)
  )
}

set.seed(51336848)
generator_minor <- SBC_generator_function(single_dataset_minor, N = 10)
datasets_minor <- generate_datasets(generator_minor, n_datasets = 200)
```

Can we see something by looking at the results of just the first 10 datasets? (note that `SBC_datasets` objects support subsetting).

```{r}
results_minor_10 <- compute_results(datasets_minor[1:10], backend_minor)
```

Not really...

```{r}
plot_ecdf_diff(results_minor_10)
```
Will we have better luck with 100 datasets? (Note that we can use `bind_results` to combine multiple results,
letting us start small, but not throw away the computation spent for the initial SBC runs)

```{r}
results_minor_100 <- bind_results(
  results_minor_10,
  compute_results(datasets_minor[11:100], backend_minor)
)
```
Here we see something suspicios with the `sigma` parameter, but it is not very convincing.

```{r}
plot_ecdf_diff(results_minor_100)
```
So let's do additional 100 SBC steps

```{r}
results_minor_200 <- bind_results(
  results_minor_100,
  compute_results(datasets_minor[101:200], backend_minor)
)
```
OK, so this looks at least a bit conclusive, but still, the violation of uniformity is not very big.

```{r}
plot_ecdf_diff(results_minor_200)
```
If we used more data points per simulation (here we simulated just 10), the problem would likely show faster.
In any case, we need a relatively large number of runs to identify small discrepancies with high probability.

But it is also the case that the estimates are not completely meaningless (as the distributions are quite close). One way to look into this is to plot the posterior mean + central 90% interval against the simulated value via `plot_sim_estimated`. The estimates should cluster around the y=x line (blue), which they mostly do.

```{r}
plot_sim_estimated(results_minor_200, alpha = 0.5)
```


# Prior mismatch

Especially when those affect only prior as SBC is based on fitted posterior - so
if prior does not influence posterior very much...

TODO

# Missing likelihood

SBC will not notice if you completely omit likelihood from your Stan model!

```{r}
single_dataset_missing <- function(N) {
  mu <- rnorm(n = 1, mean = 0, sd = 1)
  y <- rnorm(n = N, mean = mu, sd = 1)
  
  list(
    parameters = list(mu = mu),
    generated = list(N = N, y = y)
  )
}

set.seed(25746223)
generator_missing <- SBC_generator_function(single_dataset_missing, N = 10)
datasets_missing <- generate_datasets(generator_missing, n_datasets = 200)
```


```{r}
model_code_missing <- "
data {
  int<lower=0> N;
  vector[N] y;
}

parameters {
  real mu;
}

model {
  target += normal_lpdf(mu | 0, 1);
}
"

iter_warmup <- 300
iter_sampling <- 1000

if(use_cmdstanr) {
  model_missing <- cmdstan_model(write_stan_file(model_code_missing))

  backend_missing <- SBC_backend_cmdstan_sample(
    model_missing, iter_warmup = iter_warmup, iter_sampling = iter_sampling, chains = 1)
} else {
  model_missing <- stan_model(model_code = model_code_missing)

  backend_missing <- SBC_backend_rstan_sample(
    model_missing, iter = iter_sampling + iter_warmup, warmup = iter_warmup, chains = 1)
}

```
```{r}
results_missing <- compute_results(datasets_missing, backend_missing)
```

```{r}
plot_ecdf_diff(results_missing)
plot_rank_hist(results_missing)
```


Can be noticed by prior/posterior contraction plot. For this model, we can
get the prior sd directly, but one can also use a (preferably large) `SBC_datasets` object
to estimate it empirically for more complex models.

```{r}
prior_sd <- c("mu" = 1)
#prior_sd <- calculate_prior_sd(generate_datasets(generator_missing, 1000))
plot_contraction(results_missing, prior_sd)
```
We see that the contraction centers around 0 (no contraction) with some deviation (as expected due to stochasticity of the estimate), which means that the model learns
nothing useful on average about `mu`.

There is however even more powerful method - and that is to include the likelihood in the SBC.
This is most easily done by adding a "generated quantity" to the SBC results - this is a function
that is evaluated within the context of the parameters AND data.

```{r}
normal_lpdf <- function(y, mu, sigma) {
  sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))
}

log_lik_gq <- generated_quantities(log_lik = normal_lpdf(y, mu, 1), .globals = "normal_lpdf" )

results_missing_gq <- recompute_statistics(results_missing, datasets_missing, 
                                             gen_quants = log_lik_gq)
```


```{r}
plot_ecdf_diff(results_missing_gq)
plot_rank_hist(results_missing_gq)
```


# Partially missing likelihood

A more complicated case is when the likelihood is only slightly wrong (and missing something) - e.g. due to an indexing error.
Turns out missing just one data point needs a lot of steps, so we'll miss 3 in our likelihood.

```{r}
model_code_missing_2 <- "
data {
  int<lower=0> N;
  vector[N] y;
}

transformed data {
  int N2 = N / 2 + 1;
}

parameters {
  real mu;
}

model {
  target += normal_lpdf(mu | 0, 1);
  for(n in 1:N2) {
    target += normal_lpdf(y[n] | mu, 1);
  }
}
"

if(use_cmdstanr) {
  model_missing_2 <- cmdstan_model(write_stan_file(model_code_missing_2))

  backend_missing_2 <- SBC_backend_cmdstan_sample(
    model_missing_2, iter_warmup = iter_warmup, iter_sampling = iter_sampling, chains = 1)
} else {
  model_missing_2 <- stan_model(model_code = model_code_missing_2)

  backend_missing_2 <- SBC_backend_rstan_sample(
    model_missing_2, iter = iter_sampling + iter_warmup, warmup = iter_warmup, chains = 1)
}

```

```{r}
results_missing_2 <- compute_results(datasets_missing, backend_missing_2, gen_quants = log_lik_gq)
```

The contraction plot would not show anything suspicious - we get decent contraction

```{r}
plot_contraction(results_missing_2, prior_sd, parameters = "mu")
```

Now contraction is pretty high, and `mu` is behavign well, but our `log_lik` generated quantity shows a clear problem

```{r}
plot_ecdf_diff(results_missing_2)
plot_rank_hist(results_missing_2)
```

We could definitely find even smaller deviations than omitting half the data points, that would however require more SBC steps.
This boils down to the earlier discussion on small changes to the model - omitting a few data points does not change the posterior very much and thus is harder to detect by SBC - but it is possible.
