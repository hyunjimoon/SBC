---
title: "self calibration analysis and optimization"
author: "Hyunji Moon, Shinyoung Kim"
output: html_document
---

## Preface and Contents

In this vignette, we explain the concept of self-consistency and its potential application for identifying miscalibrations in inference algorithms. We also introduce an iterative process for a finding a prior region with a minimal amount of computational pathologies under a given inference algorithm.

We will give a basic explanation of self-consistency and demonstrate its usage scenario with some examples. 

## Introduction and Explanation

[Simulation Based Calibration](https://arxiv.org/pdf/1804.06788.pdf)(SBC) is often used for detecting serious computational issues. The failure of a SBC test is regarded as a result of a computationally unfaithful inference algorithm. Typically the user determines whether it's a failure by analyzing a set of graphical plots, instead of a given criterion.
```{r, echo=FALSE, fig.cap="", out.width = '70%', fig.align='center', fig.cap = "Some analysis results of SBC, courtesy of M. Modrak"}
knitr::include_graphics(path.expand("~/git_repos/hyunjimoon/SBC/vignettes/rmarkdown_images/modrak_sbc_results.png"))
```

Like the image above, we can use graphical plots to get a rough idea of the pathological details; under vs over-dispersion, under vs overestimation, and so on.
In general, we inspect the rank plots for uniformity. This is normally done by viewing the plots holistically for obvious deviations, or through quantitative methods like the chi-square test or empirical coverage metrics.
---

We propose a new test metric, based on two concepts, the Data Averaged Posterior and the self-consistency metric. Our motivations is that by taking advantage of the multiple "generate and fit" iterations of SBC, it will give us additional information to construct a more interpretable metric of miscalibration.

We first draw a sample $\theta$ from the prior. Then we draw some data given the drawn prior sample($\pi(y \ | \ \theta)$) with respect to the model's data generating process. And finally, through running inference with the simulated data we obtain some posterior distribution($\pi(\tilde{\theta} \ | \ y)$):

$$
\pi(\tilde{\theta}) = \int \pi(\tilde{\theta} \ |\ y) \pi(y \ | \ \theta) \pi(\theta) \mathrm{d}y \ \mathrm{d}\theta
$$

If we were to take the average of the expectation of the computed posterior, given a faithful inference algorithm, it should equal the expectation of the prior distribution. This is the concept of the Data Averaged Posterior(DAP), $\pi(\tilde{\theta})$.

Since we realistically can't compute the exact integral, we normally repeat the "sample-twice-and-fit" process $M$ times. Let $Y \sim f(\theta)$ denote the data generating process, and $\tilde{\theta} \sim \hat{g}(Y, P_\lambda)$ denote the inference algorithm, given some parameterized prior $P_\lambda$. $P_\lambda$ can be parameterized in whichever way, but for simplicity we'll use $P_\lambda = N(\lambda_\mu, \lambda_{var})$. Let $L$ denote the number of posterior samples drawn per iteration. We can then rewrite the DAP computation as the following:

$$
\theta_m \sim P_\lambda, \ Y_m \sim f(\theta_m), \ \tilde{\theta}_m^l \sim \hat{g}(Y_m, P_\lambda) \\
l = 1, 2, \ ... \ , L \\
m = 1, 2, \ ... \ , M
$$

In other words, for each iteration $m$ we draw a prior sample, then simulate some data through the data generating process $f$, and finally generate $l$ number of posterior samples with the inference algorithm $\hat{g}$.

And with the $l$ posterior samples, we're free to compute whatever summary statistic which matches that of the parameterized prior $P_\lambda$. Since we've used the normal prior parameterization, let's go ahead and calculate the posterior mean for each iteration:

$$
\hat{\theta}_m = \frac{1}{L} \sum^L_{l=1} \tilde{\theta}_m^l
$$
Define the computational bias, $B$, given some inference algorithm $\hat{g}$ and a parameterized prior $\lambda$ as the following:

$$
B(\hat{g}, \lambda) := E[\hat{\theta} \ | \ \theta_{1,...,M}] - \bar{\theta} = \frac{1}{M} \sum_M (\hat{\theta}_m - \theta_m)
$$
where $\bar{\theta}$ denotes the expectation of the draws from the prior, $P_{\lambda}$.

Also define the computational variance $V(\hat{g}, \lambda)$ in the same fashion:

$$
V(\hat{g}, \lambda) := \frac{1}{M} \sum_M (\hat{\theta}_m - \bar{\theta})^2
$$
This is probably familiar - it's straight from the bias-variance decomposition of the mean squared error.

So to sum it up, we're still doing the repeated "simulate and fit" thing from SBC. But instead of calculating ranks, we calculate a summary statistic which can be directly compared against the prior. And then calculated the bias and variance which can give us some information on how the inference algorithm is screwing up with this particular prior. When combined with the existing SBC tools and plots, such as the chi-square test for rank uniformity, this gives us a powerful package to identify and diagnose various miscalibrations.

## Demonstration

Okay, so we explained self-consistency and how the bias-variance decomposition can be potentially used for better identifying miscalibration. In order to demonstrate, we're going to mess with a couple of models and find prior regions where the inference algorithm behaves well and poorly. We'll then investigate how these behaviors are represented by the bias and variance metrics. And finally, we'll demonstrate an iterative update scheme whose final aim is to transport initial prior to a more satisfactory region.

---

Doing the typical library imports here. We'll be using an awesome package that helps us easily perform SBC. Check it out [here](https://github.com/hyunjimoon/SBC) :D
```{r setup, results = 'hide', warning=FALSE, message=FALSE}
library(SBC)
library(cmdstanr)
library(parallel)
library(bayesplot)
library(posterior)
library(dplyr)
library(rstan)
library(future)
library(ggpubr)
library(rstanarm)
library(ggplot2)
options(mc.cores = parallel::detectCores())
plan(multisession)
options(SBC.min_chunk_size = 5)
```

We will be using the eightschools model for demonstration:

$$
Y_i \sim \mathrm{normal}(\theta_i, \sigma_i) \\
\theta'_i \sim \mathrm{normal}(0, 1) \\
\theta_i = \theta'_i \times \tau + \mu \\
\mu \sim \mathrm{normal}(0, 5) \\
\tau \sim \mathrm{normal}(0, 5)
$$
The parameter of interest here will be $\tau$. In the original model, a prior distribution of $\mathrm{normal}(0, 5)$ has been set, but we'd like to investigate how other prior regions fare in terms of SBC. So we'll try a grid search on the prior hyperparameters with both HMC and optimization and see how the results differ.

$$
Y_i \sim \mathrm{normal}(\theta_i, \sigma_i) \\
\theta'_i \sim \mathrm{normal}(0, 1) \\
\theta_i = \theta'_i \times \tau + \mu \\
\mu \sim \mathrm{normal}(0, 5) \\
\tau \sim \mathrm{normal}(\lambda_{\mu}, \lambda_{\sigma^2})
$$
The hyperparameters for the distribution of $\mathrm{log}(\tau)$, $\lambda_{\mu}, \lambda_{\sigma^2}$, will be the subject of the grid search.

---
### Setup
We'll set the basic variables here. The number of SBC iterations($M$ in the previous notation) will be set to 100 with 1000 posterior samples($L$). We'll also be rolling with the standard 4 chain setup for HMC:

```{r, cache = TRUE}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 100

# number of observations
nobs <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 1000

# number of chains for hmc posterior approximation
nchains <- 4

fixed_args_eightschools <- list(J = 8, nsims = nsims, sigma = c(15, 10, 16, 11, 9, 11, 10, 18), nsims=nsims, dist_types=list(tau="normal"))
```

The `SBC` package allows us to define the prior and data generating process, $P_\lambda,f$, purely in R. This serves two purposes. First, we can verify that the model we have specified is consistent with our intentions i.e. contains no bugs. And we can freely simulate parameters and data without having to call Stan. 

The `generator` function is the prior and data generating process:

```{r, cache = TRUE}
generator_eightschools_ncp <- function(lambdas, fixed_args){
  # fixed value across simulated datasets
  nsims <- fixed_args$nsims
  J <- fixed_args$J
  sigma <- fixed_args$sigma
  
  lambda_mu <- lambdas$tau$mu
  lambda_sigma <- sqrt(lambdas$tau$var)
  
  # Draw tau from the designated normal distribution
  # lambda_sigma is positively bounded
  tau <- abs(rnorm(1, lambda_mu, lambda_sigma))
  
  # other parameters are drawn from the default prior
  mu = rnorm(1, 0, 5)
  
  theta_trans <- rnorm(J, 0, 1)
  
  theta <- theta_trans * tau + mu
  # draw y from simulated parameters
  y <- rnorm(J, theta, sigma)
  
  list(
    variables = list(
      tau = tau
      ), 
    generated = list(
      J = J,
      y = y,
      sigma = sigma,
      nsims = nsims,
      lambda_mu = lambda_mu,
      lambda_var = lambdas$tau$var
      )
  )
}
```

Note the code line ` tau <- rnorm(1, lambda_mu, lambda_sigma)`. Like exactly in the model specification, we are drawing $\mathrm{log}(\tau)$ given hyperparameters $\lambda_{\mu}, \lambda_{\sigma^2}$.

Now we define the HMC and optimizing `SBC` backends, which represent $\hat{g}$.

```{R, warning=FALSE, message=FALSE, error=FALSE}
cmdstan_mod_eightschools <- cmdstanr::cmdstan_model("models/eightschools_ncp_posteriordb.stan")
backend_eightschools_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_eightschools, chains = 4, iter_sampling = ndraws / 4)

rstan_mod_eightschools <- rstan::stan_model("models/eightschools_ncp_posteriordb.stan")
backend_eightschools_opt <- SBC_backend_rstan_optimizing(rstan_mod_eightschools, draws=ndraws)

calib_generator <- function(lambdas, fixed_args){
  generate_datasets(SBC_generator_function(generator_eightschools_ncp, lambdas, fixed_args), n_datasets = fixed_args_eightschools$nsims)
}
```

Finally we define a function that computes the DAP and then calculate bias, variance metrics:

```{r, cache = TRUE}
calculate_dap <- function(lambda_mu, lambda_var, generator, datasets=NULL, backened, fixed_args){
  if(is.null(datasets)){
      lambda_init_eightschools <- list(
        tau = list(mu=lambda_mu, var=lambda_var)
      )
      datasets <- do.call(generator, list(lambda_init_eightschools, fixed_args = fixed_args))
  }
  sbc_result <- compute_results(datasets, backened, thin_ranks = 1)
  draws_params <- c()
  draws_Y <- c()
  prior_thetas <- posterior::extract_variable(datasets$variables, "tau")
  theta_bar <- mean(prior_thetas)
  
  B <- 0
  V <- 0
  for(i in 1:nsims){
    draws_Y <- c(draws_Y, datasets$generated[[i]]$Y)
    samples <- SBC_fit_to_draws_matrix(sbc_result$fits[[i]])
    params <- posterior::extract_variable(samples, "tau")
    draws_params <- c(draws_params, params)
    
    B <- B + mean(params)
    V <- V + sum((params - mean(params))^2)
  }
  
  B <- B / fixed_args$nsims - theta_bar
  V <- V / (fixed_args$nsims * ndraws)
  
  mu <- mean(draws_params)
  var <- sd(draws_params)^2
  
  return(list(mu=mu, var=var, draws_params=draws_params, draws_Y=draws_Y, B=B, V=V, datasets=datasets))
}
```

Once we have the model and DAP generation we can start the grid search. We create a grid of mean-variance values and run inference with HMC and optimization for each hyperparameter combination. Note that the same prior-data samples are being used for optimization and HMC:

```{r include=FALSE, eval=TRUE}
mu_seq <- readRDS(file="DAP_self_calibration_mu_seq.rds")
var_seq <- readRDS(file="DAP_self_calibration_var_seq.rds")
grid_size <- length(mu_seq) * length(var_seq)

dap_lambda_mu <- readRDS(file="DAP_self_calibration_dap_lambda_mu.rds")
dap_lambda_var <- readRDS(file="DAP_self_calibration_dap_lambda_var.rds")
lambda_mu <- readRDS(file="DAP_self_calibration_lambda_mu.rds")
lambda_var <- readRDS(file="DAP_self_calibration_lambda_var.rds")
B_optim <- readRDS(file="DAP_self_calibration_B_optim.rds")
V_optim <- readRDS(file="DAP_self_calibration_V_optim.rds")
B_hmc <- readRDS(file="DAP_self_calibration_B_hmc.rds")
V_hmc <- readRDS(file="DAP_self_calibration_V_hmc.rds")
dap_lambda_mu_hmc <- readRDS(file="DAP_self_calibration_dap_lambda_mu_hmc.rds")
dap_lambda_var_hmc <- readRDS(file="DAP_self_calibration_dap_lambda_var_hmc.rds")
```

```{r, cache=TRUE, warning=FALSE, message=FALSE, eval=FALSE}
gridsize_mu <- 5
gridsize_var <- 5
mu_seq <- c(0, 1, 2, 3, 4)
var_seq <- c(1, 2, 5, 7, 10)
grid_size <- length(mu_seq) * length(var_seq)

dap_lambda_mu <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
dap_lambda_var <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
rownames(dap_lambda_mu) <- mu_seq
rownames(dap_lambda_var) <- mu_seq
colnames(dap_lambda_mu) <- var_seq
colnames(dap_lambda_var) <- var_seq

lambda_mu <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
lambda_var <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
rownames(lambda_mu) <- mu_seq
rownames(lambda_var) <- mu_seq
colnames(lambda_mu) <- var_seq
colnames(lambda_var) <- var_seq

B_optim <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
V_optim <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
rownames(B_optim) <- mu_seq
rownames(V_optim) <- mu_seq
colnames(B_optim) <- var_seq
colnames(V_optim) <- var_seq

B_hmc <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
V_hmc <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
rownames(B_hmc) <- mu_seq
rownames(V_hmc) <- mu_seq
colnames(B_hmc) <- var_seq
colnames(V_hmc) <- var_seq

dap_lambda_mu_hmc <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
dap_lambda_var_hmc <- array(rep(NA, grid_size), dim = c(gridsize_mu, gridsize_var))
rownames(dap_lambda_mu_hmc) <- mu_seq
rownames(dap_lambda_var_hmc) <- mu_seq
colnames(dap_lambda_mu_hmc) <- var_seq
colnames(dap_lambda_var_hmc) <- var_seq

for(j in 1:length(var_seq)){
  for(i in 1:length(mu_seq)){
    dap <- calculate_dap(mu_seq[[i]],var_seq[[j]] , calib_generator, NULL, backend_eightschools_opt, fixed_args_eightschools)
    dap_lambda_mu[i, j] <- dap$mu
    dap_lambda_var[i, j] <- dap$var
    lambda_mu[i, j] <- mu_seq[[i]]
    lambda_var[i, j] <- var_seq[[j]]
    B_optim[i, j] <- dap$B
    V_optim[i, j] <- dap$V
    
    dap_hmc <- calculate_dap(mu_seq[[i]],var_seq[[j]], calib_generator, dap$datasets, backend_eightschools_hmc, fixed_args_eightschools)
    B_hmc[i, j] <- dap_hmc$B
    V_hmc[i, j] <- dap_hmc$V
    dap_lambda_mu_hmc[i, j] <- dap_hmc$mu
    dap_lambda_var_hmc[i, j] <- dap_hmc$var
  }
}
```

```{r, include=FALSE, eval=FALSE}
saveRDS(mu_seq, file="DAP_self_calibration_mu_seq.rds")
saveRDS(var_seq, file="DAP_self_calibration_var_seq.rds")

saveRDS(dap_lambda_mu, file="DAP_self_calibration_dap_lambda_mu.rds")
saveRDS(dap_lambda_var, file="DAP_self_calibration_dap_lambda_var.rds")
saveRDS(lambda_mu, file="DAP_self_calibration_lambda_mu.rds")
saveRDS(lambda_var, file="DAP_self_calibration_lambda_var.rds")
saveRDS(B_optim, file="DAP_self_calibration_B_optim.rds")
saveRDS(V_optim, file="DAP_self_calibration_V_optim.rds")
saveRDS(B_hmc, file="DAP_self_calibration_B_hmc.rds")
saveRDS(V_hmc, file="DAP_self_calibration_V_hmc.rds")
saveRDS(dap_lambda_mu_hmc, file="DAP_self_calibration_dap_lambda_mu_hmc.rds")
saveRDS(dap_lambda_var_hmc, file="DAP_self_calibration_dap_lambda_var_hmc.rds")

```

After waiting quite a bit for the computation to finish, we can compare the results between HMC and optimization. Let's first look at optimization's prior plot:

```{r, cache=TRUE, fig.align='center', fig.cap="prior-posterior grid plot of optimization"}

scaleFUN <- function(x) sprintf("%.2f", x)
dap_df <- as.data.frame.table(dap_lambda_mu)
colnames(dap_df)[1] <- "mu"
colnames(dap_df)[2] <- "var"
dap_df[, "mu"] <- as.numeric(as.vector(dap_df[, "mu"]))  # not using as.vector converts the factor indices
dap_df[, "var"] <- as.numeric(as.vector(dap_df[, "var"]))
colnames(dap_df)[3] <- "dap_mu"
dap_lambda_df <- as.data.frame.table(dap_lambda_var)
dap_df[, "dap_var"] <- as.numeric(dap_lambda_df$Freq)
ggplot(dap_df, mapping=aes(x="mu", y="var")) + geom_point(aes(x=mu, y=var), color = "red") + geom_point(aes(x=dap_mu, y = dap_var), color = "blue") + scale_y_continuous(labels=scaleFUN) + geom_segment(aes(x=mu, y=var, xend=dap_mu, yend=dap_var), size=0.2, arrow = arrow(length = unit(0.3, "cm"))) + xlab("mean") + ylab("variance") + ggtitle("optimization")
```

The red points on the bottom right denote the initial hyperparameter values; variance plotted against mean. The blue points are the same statistics calculated against samples from the DAP. If a given prior was to be self-consistent, we should expect little discrepancy between the initial prior values and the DAP. We can see that some priors show very large variance values recovered from the DAP, which is assumed to be caused from the combination of an extreme prior and optimization not finding the optima.

Let's zoom in to identify which prior combinations are being problematic:

```{R, fig.align='center', fig.cap="zoomed prior-posterior grid plot of HMC"}
ggplot(dap_df, mapping=aes(x="mu", y="var")) + geom_point(aes(x=mu, y=var), color = "red") + geom_point(aes(x=dap_mu, y = dap_var), color = "blue") + scale_y_continuous(labels=scaleFUN) + geom_segment(aes(x=mu, y=var, xend=dap_mu, yend=dap_var), size=0.2, arrow = arrow(length = unit(0.3, "cm"))) + xlab("mean") + ylab("variance") + ggtitle("optimization") + coord_cartesian(ylim=c(0, 20), xlim = c(0, 5)) + scale_y_continuous(breaks=c(1:20))
```
For priors with mean value zero, DAP values couldn't be calculated, resulting in `Inf`. Other priors show the recovered DAP variance values to be spurious and extremely large.


Let's see if HMC fares better:

```{r, cache=TRUE, fig.align='center', fig.cap="prior-posterior grid plot of HMC"}

scaleFUN <- function(x) sprintf("%.2f", x)
dap_df <- as.data.frame.table(dap_lambda_mu_hmc)
colnames(dap_df)[1] <- "mu"
colnames(dap_df)[2] <- "var"
dap_df[, "mu"] <- as.numeric(as.vector(dap_df[, "mu"]))  # not using as.vector converts the factor indices
dap_df[, "var"] <- as.numeric(as.vector(dap_df[, "var"]))
colnames(dap_df)[3] <- "dap_mu"
dap_lambda_df <- as.data.frame.table(dap_lambda_var_hmc)
dap_df[, "dap_var"] <- as.numeric(dap_lambda_df$Freq)
ggplot(dap_df, mapping=aes(x="mu", y="var")) + geom_point(aes(x=mu, y=var), color = "red") + geom_point(aes(x=dap_mu, y = dap_var), color = "blue") + scale_y_continuous(labels=scaleFUN, breaks=c(1:10)) + geom_segment(aes(x=mu, y=var, xend=dap_mu, yend=dap_var), size=0.2, arrow = arrow(length = unit(0.3, "cm"))) + xlab("mean") + ylab("variance") + ggtitle("HMC")
```

Although to a lesser degree, we can still observe some deviations from the prior. But overall, HMC performs better inference preserving self-calibration compared to optimization. The deviations aren't due to sampling errors but instead the posterior moving toward regions favoring the likelihood, since we're using `sigma` values from the original model's data, instead of simulated values.

We'll now plot the bias and variance metrics of optimization and HMC together. Since HMC was relatively well calibrated as to optimization, we should observe its metrics being lower than that of optimization:

```{r, cache=TRUE, fig.width = 14, fig.align='center'}

B_optim_df <- as.data.frame.table(B_optim)
colnames(B_optim_df)[colnames(B_optim_df) == "Var1"] <- "Mu"
colnames(B_optim_df)[colnames(B_optim_df) == "Var2"] <- "Var"
colnames(B_optim_df)[colnames(B_optim_df) == "Freq"] <- "B"
B_optim_df[, "type"] = "optimization"

B_hmc_df <- as.data.frame.table(B_hmc)
colnames(B_hmc_df)[colnames(B_hmc_df) == "Var1"] <- "Mu"
colnames(B_hmc_df)[colnames(B_hmc_df) == "Var2"] <- "Var"
colnames(B_hmc_df)[colnames(B_hmc_df) == "Freq"] <- "B"
B_hmc_df[, "type"] = "hmc"

B_df = rbind(B_optim_df, B_hmc_df)
B_df[, "B"] <- abs(B_df[, "B"])

breaks <- c(-0.1, 0.01, 0.1, 1, 2, 4, 16)

ggplot(na.omit(B_df)) + geom_tile(aes(Mu, Var, fill=B)) + coord_fixed() + facet_grid(cols=vars(type)) + scale_fill_gradient2(low = "blue", mid="white", high = "red", trans="log", breaks=breaks) + ggtitle("abs(B) plot for HMC and optimization (lower is better)")
```
Note that bias values for optimization at mean = 0 was unable to be calculated, since optimization returned extreme results.

We can observe that HMC possesses far smaller bias metric values, implying that HMC contains less bias within its inference results as to optimization. In addition, some prior regions, such as for cases where `Var <= 2` and `Mu >= 2` yields the lowest bias values, suggesting that these regions are preferable as to other regions for minimal bias.

```{r, cache=TRUE, fig.width = 14, fig.align='center'}

V_optim_df <- as.data.frame.table(V_optim)
colnames(V_optim_df)[colnames(V_optim_df) == "Var1"] <- "Mu"
colnames(V_optim_df)[colnames(V_optim_df) == "Var2"] <- "Var"
colnames(V_optim_df)[colnames(V_optim_df) == "Freq"] <- "V"
V_optim_df[, "type"] = "optimization"

V_hmc_df <- as.data.frame.table(V_hmc)
colnames(V_hmc_df)[colnames(V_hmc_df) == "Var1"] <- "Mu"
colnames(V_hmc_df)[colnames(V_hmc_df) == "Var2"] <- "Var"
colnames(V_hmc_df)[colnames(V_hmc_df) == "Freq"] <- "V"
V_hmc_df[, "type"] = "hmc"

V_df = rbind(V_optim_df, V_hmc_df)
V_df[, "V"] <- abs(V_df[, "V"])

ggplot(na.omit(V_df)) + geom_tile(aes(Mu, Var, fill=V)) + coord_fixed() + facet_grid(cols=vars(type)) + scale_fill_gradient2(low = "blue", mid="white", high = "red", trans="log") + ggtitle("abs(V) plot for HMC and optimization (lower is better)")
```

For the variance metric $V$, we can again see that HMC outperforms optimization, showing that the dispersion of recovered posterior values are far lower, suggesting a much lower level of error as to optimization.

### Iteratively updating prior for well-calibration

Up to now, we've worked with metrics and identified various regions where miscalibration occured, or on the other side, well-calibrated. Can we try and go the other direction? How can we use the prior-posterior discrepancy information to refine our prior? In this section, we explore a prior update algorithm which attempts to move from a badly calibrated region to a more well-calibrated area.

The iterative update algorithm uses the metric attained from a calculated DAP to update the prior's hyperparameters. Recall the DAP equation:

$$
\pi(\tilde{\theta}) = \int \pi(\tilde{\theta} \ |\ y) \pi(y \ | \ \theta) \pi(\theta) \mathrm{d}y \ \mathrm{d}\theta
$$

We generate metric from the DAP $\pi(\tilde{\theta})$, which allows us to directly measure the discrepancy between it and the prior distribution's hyperparameters, $\pi(\theta) = P_\lambda$. The discrepency can be used to guide a local search along the prior space. The increment of the descent step can be varied, but we provide a few options:

1. "Markov chain"-like: This method reuses the calculated summary statistics of the DAP as the prior hyperparameter values of the next iterations. It's the simplest option, but can be effective for simpler models.
2. Cubic weights: We increment/decrement the hyperparameters in cubic order so that as the discrepancy gets smaller, the stepsize also gets smaller. On the otherhand, large discrepency can result in exponential stepsizes which causes the hyperparameters to rapidly shrink towards the DAP summary statistic. It was found that this method was effective in speeding up calibration for priors with a relatively scale-invariant region, since the cubic nature may only allow subtle movements near the ambient region of the DAP statistic.

We'll try to start from a "bad", miscalibrated prior region and try to iteratively refine the prior into a more favorable region. We'll be starting from $N(2, 10)$ with optimization. Let's generate a rank plot as a quick check to see how bad the region is:
```{R, warning=FALSE, error=FALSE}
lambda_init_eightschools <- list(
  tau = list(mu=5, var=10)
)

datasets_eightschools_new <- generate_datasets(SBC_generator_function(generator_eightschools_ncp, lambda_init_eightschools, fixed_args_eightschools), n_sims = fixed_args_eightschools$nsims)

result_eightschools_opt_new <- compute_results(datasets_eightschools_new, backend_eightschools_opt, thin_ranks = 1)
    
plot_rank_hist(result_eightschools_opt_new, variables=list("tau"))
```

We can see that the ranks are not uniform, and thus the starting point is not well calibrated. Let's try the iterative update:

```{R, warning=FALSE, message=FALSE}
updator = "mc_update"


# tolerance
tol <- 0.01

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update

sc_eightschools_opt <- self_calib_adaptive(calib_generator, backend_eightschools_opt, updator, c("tau"), lambda_init_eightschools, nsims, 15, gamma, tol, fixed_args = fixed_args_eightschools)
```

This algorithm continuously updates the prior with the previous iteration's posterior, which drives the posterior into a well-calibrated region.

```{R, warning=FALSE, error=FALSE}
sc_eightschools_opt$t_df
```

The iterative update algorithm determined the following $\mu, \sigma^2$ prior to be better calibrated:

```{r}
sc_eightschools_opt$lambda$var = sc_eightschools_opt$lambda$var ^ 2
sc_eightschools_opt$lambda
```

Let's check the rank plots at the region:


```{R, warning=FALSE, error=FALSE}
datasets_eightschools_new <- generate_datasets(SBC_generator_function(generator_eightschools_ncp,  sc_eightschools_opt$lambda, fixed_args_eightschools), n_sims = fixed_args_eightschools$nsims)

result_eightschools_opt_new <- compute_results(datasets_eightschools_new, backend_eightschools_opt, thin_ranks = 1)
    
plot_rank_hist(result_eightschools_opt_new, variables=list("tau"))
```

We can identify that the rank plot has been significantly improved, which suggests the refined prior may be considered for usage if self-calibration is desired.

We'll try again with HMC this time:


```{R, warning=FALSE, message=FALSE}
updator = "mc_update"


# tolerance
tol <- 0.01

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update

sc_eightschools_opt <- self_calib_adaptive(calib_generator, backend_eightschools_hmc, updator, c("tau"), lambda_init_eightschools, nsims, 10, gamma, tol, fixed_args = fixed_args_eightschools)

datasets_eightschools_new <- generate_datasets(SBC_generator_function(generator_eightschools_ncp,  sc_eightschools_opt$lambda, fixed_args_eightschools), n_sims = fixed_args_eightschools$nsims)

result_eightschools_opt_new <- compute_results(datasets_eightschools_new, backend_eightschools_opt, thin_ranks = 1)
    
plot_rank_hist(result_eightschools_opt_new, variables=list("tau"))
```
## SBC on Binom

```{R, warning=FALSE, message=FALSE}
nsims <- 200

# number of observations
nobs <- 10#2

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2

generator_binom <- function(lambdas, fixed_args){
  # fixed value across simulated datasets
  # experiment settings
  nobs <- fixed_args$nobs
  nsize <- fixed_args$nsize
  dist_types <- fixed_args$dist_types
  # modular settings
  link_type <- fixed_args$link_type
  
  # generate
  lambda_arg1 <- c()
  lambda_arg2 <- c()
  if(dist_types$eta == "normal"){
    eta <- rnorm(1, mean = lambdas$eta$mu, sd=lambdas$eta$sigma)
    lambda_arg1 <- c(lambda_arg1, lambdas$eta$mu)
    lambda_arg2 <- c(lambda_arg2, lambdas$eta$sigma)
  }
  else if(dist_types$eta == "gamma"){
    eta <- rgamma(1, shape = lambdas$eta$alpha, rate = lambdas$eta$beta)
    lambda_arg1 <- c(lambda_arg1, lambdas$eta$alpha)
    lambda_arg2 <- c(lambda_arg2, lambdas$eta$beta)
  }
  
  
    
  mu <- invtf_param_vec(eta, link_type = link_type)
  Y <- rbinom(nobs, size = nsize, prob = mu) 
  list(
    parameters = list(eta = eta),
    generated = list(nobs= nobs, nsize = nsize, link = link_type,
                     dist_types = match(unlist(dist_types), c("normal", "gamma")), lambda_arg1 = lambda_arg1, lambda_arg2 = lambda_arg2, 
                     Y = Y)
  )
}

fixed_args_binom <- list(nobs = nobs, nsize = nsize, link_type = 1, nsims = nsims, ndraws = ndraws, dist_types=list(eta="normal"))
```

```{R, warning=FALSE, error=FALSE}
## Generator settings
# number of SBC simulations per iteration (generator)


lambda_init_binom <- list(
  eta = list(mu=100, sigma=100)
)
datasets_binom <- SBC::generate_datasets(SBC_generator_function(generator_binom, lambda_init_binom, fixed_args_binom), n_datasets = fixed_args_binom$nsims)

# hyperparameter update algorithm 
updator = "mc_update"

# maximal number of SBC iterations
niter <- 100

# tolerance
tol <- 0.1

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update

# step2: inferring posterior
rstan_binom_mod <- stan_model("models/binom-laplace.stan")
cmdstan_binom_mod <- cmdstanr::cmdstan_model("models/binom-laplace.stan")

backend_binom_opt <- SBC_backend_rstan_optimizing(rstan_binom_mod, draws = ndraws)
#backend_binom_hmc <- SBC_backend_cmdstan_sample(cmdstan_binom_mod, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
#result_binom_opt <- compute_results(datasets_binom, backend_binom_opt, thin_ranks = 1)

```

HMC on the otherhand couldn't converge to a well-calibrated region.

## The role of the calibration algorithm and the interpretation of calibrated $\lambda$

The calibration algorithm iteratively updates the prior, $\lambda$, in an attempt to move to a calibrated region. This means That its hyperparameter values, mean and variance in the case of the Normal distribution, are the target of the update. We would like to address the statement of whether basic priors are sufficient in representing computationally calibrated priors.

In the case of the sequential update algorithm, the goal is to allocate high probability mass on desirable regions, and vice versa. The resulting calibrated only conveys the following information:  It was found that the region centered on the resulting distribution was (relatively) well calibrated.

We would believe that in most cases the calibrated prior will not be used directly; instead it will be used as a reference for determining the bounds of the calibrated region. Given this information, domain knowledge will be elicited into an actual prior distribution, while hopefully abiding the calibrated region information.

In conclusion, finding calibrated regions does not involve domain knowledge elicitation and in most cases the simple probability densities may be sufficient just for identifying calibrated regions of the prior. The user then may use this range of values as a reference when defining priors in an attempt for a computationally calibrated prior.

In addition, we have demonstrated that it struggles in highly sensitive and difficult prior regions; this method may not be preferable for models in which prior influence trumps that of the likelihood.

## Wrap-up

We've explored the concept of self-consistency and with prior-posterior grid plots identified prior regions which are deemed to be miscalibrated. Furthermore, we've used two metric, the bias and variance calibration metrics to identify in detail how different prior regions behave with respect to an inference algorithm. 

Finally, we showed by iteratively updating the prior we were able to go from a poorly calibrated region to a well-calibrated region. The hyperparameters of the prior were sequentially updated until relative calibration was reached, giving the user some reference point of a calibrated region to aid in prior construction.
