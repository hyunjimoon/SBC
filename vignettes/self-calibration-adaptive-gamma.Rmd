---
title: "self-calibration-adaptive for Gamma regression"
author: "Hyunji Moon, Shinyoung Kim"
output:
  html_document: default
  pdf_document: default
---
```{r setup, include=FALSE, warning=FALSE}
library(SBC)
library(cmdstanr)
library(parallel)
library(bayesplot)
library(posterior)
library(dplyr)
library(rstan)
library(future)
library(ggpubr)
library(rstanarm)
library(ggplot2)
library(mclust)
options(mc.cores = parallel::detectCores())
#set.seed(1984)
```

```{R, warning=FALSE, error=FALSE}
## Generator settings
# number of SBC simulations per iteration (generator)
nsims <- 100

# number of observations
nobs <- 100

# link function (1 = logit, 2 = probit, 3 = cloglog)
link <- 1

# number of binomial trials per observation
nsize <- 10

## Backend settings
# number of draws per posterior approximation 
ndraws <- 100

# number of chains for hmc posterior approximation
nchains <- 2
```

# Inspecting DAP of gamma-regression-hmc

We will calibrate the `shape` parameter, by changing the prior distribution for `shape`, as well as `a`

```{R, warning=FALSE, error=FALSE}
generator_gr <- function(lambdas, fixed_args){
  # fixed value across simulated datasets
  ## meta
  nobs <- fixed_args$nobs
  K <- fixed_args$K
  dist_types <- fixed_args$dist_types
  while(TRUE){
    # predictor
    X <- array(rnorm(nobs * K, mean = 1, sd = 0.5), dim = c(nobs, K))
    b <- rnorm(K, mean = 0, sd = 1)
    # generate
    lambda_arg1 <- c()
    lambda_arg2 <- c()
    if(dist_types$shape == "normal"){
      shape <- rnorm(1, mean = lambdas$shape$mu, sd=lambdas$shape$sigma)
      lambda_arg1 <- c(lambda_arg1, lambdas$shape$mu)
      lambda_arg2 <- c(lambda_arg2, lambdas$shape$sigma)
    }
    else if(dist_types$shape == "gamma"){
      shape <- rgamma(1, shape = lambdas$shape$alpha, rate = lambdas$shape$beta)
      lambda_arg1 <- c(lambda_arg1, lambdas$shape$alpha)
      lambda_arg2 <- c(lambda_arg2, lambdas$shape$beta)
    }
    
    if(dist_types$a == "normal"){
      a <- rnorm(1, mean = lambdas$a$mu, sd=lambdas$a$sigma)
      lambda_arg1 <- c(lambda_arg1, lambdas$a$mu)
      lambda_arg2 <- c(lambda_arg2, lambdas$a$sigma)
    }
    else if(dist_types$a == "gamma"){
      a <- rgamma(1, shape = lambdas$a$alpha, rate = lambdas$a$beta)
      lambda_arg1 <- c(lambda_arg1, lambdas$a$alpha)
      lambda_arg2 <- c(lambda_arg2, lambdas$a$beta)
    }
    
    #a <- rnorm(1, mean = 2, sd = 5)
    logmu <- as.numeric(a + X %*% b)
    mu <- exp(logmu)
    Y <- rgamma(nobs, shape = shape, rate = shape / mu)
    if(!any(Y <= 1e-32)){
      return(list(
              parameters = list(shape = shape),
              generated = list(nobs= nobs, K = K, X = X, dist_types = match(unlist(dist_types), c("normal", "gamma")), 
                               lambda_arg1 = lambda_arg1, lambda_arg2 = lambda_arg2, Y = Y)
              )
             )
    }
  }
}
fixed_args_gr <- list(nobs = nobs, K = 15, nsims = nsims, dist_types=list(shape="gamma", a="normal"))
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
rstan_mod_gr <- stan_model("models/gamma-reg.stan")
backend_gr_opt <- SBC_backend_rstan_optimizing(rstan_mod_gr, draws = ndraws)
backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = nchains, iter_sampling = ndraws / nchains) # thin = 10
```

We start at an initially badly calibrated region:

```{R, warning=FALSE, error=FALSE}
updator = "mc_update"

# maximal number of SBC iterations
niter <- 100

# tolerance
tol <- 0.1

# learning rate
gamma <- 1.5 # 0.5 for gradient update, 10 for normal_str_update
nobs <- 10
# prior hyperparameters
lambda_init_gamma <- list(
  shape = list(alpha=2, beta=1),
  a = list(mu=2, sigma=1)
)
fixed_args_gamma <- list(nobs = nobs, K = 15, nsims = nsims, dist_types=list(shape="gamma", a="normal"))
datasets_gamma <- generate_datasets(SBC_generator_function(generator_gr, lambda_init_gamma, fixed_args_gamma), n_datasets = nsims)
# combines target hp values with other hyperparameter settings
calib_generator <- function(lambda_init_gamma, fixed_args){
  generate_datasets(SBC_generator_function(generator_gr, lambda_init_gamma, fixed_args), n_datasets = fixed_args_gamma$nsims)
}
#rstan_mod_gr <- stan_model("models/gamma-reg.stan")
cmdstan_mod_gr <- cmdstanr::cmdstan_model("models/gamma-reg.stan")
# backend_gr_opt <- SBC_backend_rstan_optimizing(rstan_mod_gr, draws = ndraws)
# OPT DOES NOT WORK: result in Error in chol.default(-H) : the leading minor of order 13 is not positive definite

backend_gr_hmc <- SBC_backend_cmdstan_sample(cmdstan_mod_gr, chains = 4, iter_sampling = ndraws / 4) # thin = 10

# initial badly calibrated
result_gr_hmc <- compute_results(datasets_gamma, backend_gr_hmc)
plot_rank_hist(result_gr_hmc)
```



```{R, warning = FALSE, error = FALSE}
calculate_dap <- function(mu, var, generator, backened, fixed_args){
  lambda_init_gamma <- list(
    shape = list(alpha= mu^2 / var, beta= mu / var),
    a = list(mu=2, var=1)
  )
  datasets <- do.call(generator, list(lambda_init_gamma, fixed_args = fixed_args))
  sbc_result <- compute_results(datasets, backened, thin_ranks = 1)
  draws_eta <- c()
  for(fit in sbc_result$fits){
    samples <- SBC_fit_to_draws_matrix(fit)
    draws_eta <- c(draws_eta, posterior::extract_variable(samples, "shape"))
  }
  # assume normal for dap
  # mu <- mean(draws_eta)
  # var <- sd(draws_eta)^2
  gamma_est <- MASS::fitdistr(draws_eta, "gamma", start=list(shape=1, rate=1))$estimate
  alpha <- as.numeric(gamma_est["shape"])
  beta <- as.numeric(gamma_est["rate"])
  mu = alpha / beta
  var = alpha / beta^2
  return(list(mu=mu, var=var, draws_eta=draws_eta))
}

prior_dap <- list(mu = c(), var = c(), dap_mu = c(), dap_var = c(), mu_loss = c(), var_loss = c())
for (mu in seq(.1, .5, length.out = 4)){
  for(var in seq(.1, .5, length.out = 4)){
    prior_dap$mu <- c(prior_dap$mu, mu)
    prior_dap$var <- c(prior_dap$var, var)
    dap <- calculate_dap(mu, var, calib_generator, backend_gr_opt, fixed_args_gr)
    prior_dap$dap_mu <- c(prior_dap$dap_mu, dap$mu)
    prior_dap$dap_var <- c(prior_dap$dap_var, dap$var)
    prior_dap$mu_loss <- c(prior_dap$mu_loss, mu - dap$mu)
    prior_dap$var_loss <- c(prior_dap$var_loss, var - dap$var)
  }
}
prior_dap <- data.frame(prior_dap)
ggplot(prior_dap)+  geom_point(aes(x=mu, y=var), color = "red") + geom_point(aes(x=dap_mu, y = dap_var), color = "blue")
ggplot(prior_dap_b) + geom_density_2d(aes(x=mu, y = sigma)) + geom_density_2d(aes(x=dap_mu, y = dap_sigma), color = "red")
library(plot3D)
persp3D(x=prior_dap$dap_mu, y=sigma_vals, z = convex_v, theta=30, phi=50, xlab="mu", ylab="var", zlab="convex")
```

We now run self-calibration

```{R, warning=FALSE, error=FALSE}
knitr::opts_chunk$set(error = TRUE)
sc_opt_gr <- self_calib_adaptive(calib_generator, backend_gr_hmc, updator, c("shape", "a"), lambda_init_gamma, nsims, gamma, tol, fixed_args = fixed_args_gamma)
```

### Iteration details:

```{R, warning=FALSE, error=FALSE}
sc_opt_gr$t_df
```

```{R, warning=FALSE, error=FALSE}
# after calibration
datasets_gr_new <- generate_datasets(SBC_generator_function(generator_gr, sc_opt_gr$lambda, fixed_args_gamma), n_datasets = fixed_args_gamma$nsims)
result_gr_hmc_new <- compute_results(datasets_gr_new, backend_gr_hmc)
    
plot_rank_hist(result_gr_hmc_new)
ggplot(sc_opt_gr$t_df) + geom_point(aes(x=iter, y=shape_lambda_loss))
```
